{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jZo2bHH-rFtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必需的库\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import os"
      ],
      "metadata": {
        "id": "-I5xKoMJrF7O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "RLdmRYLVrd_9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Pre-activation ResNet in PyTorch.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    '''Pre-activation version of the original Bottleneck module.'''\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = self.conv3(F.relu(self.bn3(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.relu(self.bn(out))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(num_classes=10):\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], num_classes=num_classes)\n",
        "\n",
        "def PreActResNet34():\n",
        "    return PreActResNet(PreActBlock, [3,4,6,3])\n",
        "\n",
        "def PreActResNet50():\n",
        "    return PreActResNet(PreActBottleneck, [3,4,6,3])\n",
        "\n",
        "def PreActResNet101():\n",
        "    return PreActResNet(PreActBottleneck, [3,4,23,3])\n",
        "\n",
        "def PreActResNet152():\n",
        "    return PreActResNet(PreActBottleneck, [3,8,36,3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = PreActResNet18()\n",
        "    y = net((torch.randn(1,3,32,32)))\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ],
      "metadata": {
        "id": "1rWzyepsrhuJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "################################################################\n",
        "## Components from https://github.com/davidcpage/cifar10-fast ##\n",
        "################################################################\n",
        "\n",
        "#####################\n",
        "## data preprocessing\n",
        "#####################\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465) # equals np.mean(train_set.train_data, axis=(0,1,2))/255\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616) # equals np.std(train_set.train_data, axis=(0,1,2))/255\n",
        "\n",
        "def normalise(x, mean=cifar10_mean, std=cifar10_std):\n",
        "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
        "    x -= mean*255\n",
        "    x *= 1.0/(255*std)\n",
        "    return x\n",
        "\n",
        "def pad(x, border=4):\n",
        "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "def transpose(x, source='NHWC', target='NCHW'):\n",
        "    return x.transpose([source.index(d) for d in target])\n",
        "\n",
        "#####################\n",
        "## data augmentation\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        return x[:,y0:y0+self.h,x0:x0+self.w]\n",
        "\n",
        "    def options(self, x_shape):\n",
        "        C, H, W = x_shape\n",
        "        return {'x0': range(W+1-self.w), 'y0': range(H+1-self.h)}\n",
        "\n",
        "    def output_shape(self, x_shape):\n",
        "        C, H, W = x_shape\n",
        "        return (C, self.h, self.w)\n",
        "\n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def __call__(self, x, choice):\n",
        "        return x[:, :, ::-1].copy() if choice else x\n",
        "\n",
        "    def options(self, x_shape):\n",
        "        return {'choice': [True, False]}\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        x = x.copy()\n",
        "        x[:,y0:y0+self.h,x0:x0+self.w].fill(0.0)\n",
        "        return x\n",
        "\n",
        "    def options(self, x_shape):\n",
        "        C, H, W = x_shape\n",
        "        return {'x0': range(W+1-self.w), 'y0': range(H+1-self.h)}\n",
        "\n",
        "\n",
        "class Transform():\n",
        "    def __init__(self, dataset, transforms):\n",
        "        self.dataset, self.transforms = dataset, transforms\n",
        "        self.choices = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, labels = self.dataset[index]\n",
        "        for choices, f in zip(self.choices, self.transforms):\n",
        "            args = {k: v[index] for (k,v) in choices.items()}\n",
        "            data = f(data, **args)\n",
        "        return data, labels\n",
        "\n",
        "    def set_random_choices(self):\n",
        "        self.choices = []\n",
        "        x_shape = self.dataset[0][0].shape\n",
        "        N = len(self)\n",
        "        for t in self.transforms:\n",
        "            options = t.options(x_shape)\n",
        "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape\n",
        "            self.choices.append({k:np.random.choice(v, size=N) for (k,v) in options.items()})\n",
        "\n",
        "#####################\n",
        "## dataset\n",
        "#####################\n",
        "\n",
        "def cifar10(root):\n",
        "    train_set = torchvision.datasets.CIFAR10(root=root, train=True, download=True)\n",
        "    test_set = torchvision.datasets.CIFAR10(root=root, train=False, download=True)\n",
        "    return {\n",
        "        'train': {'data': train_set.data, 'labels': train_set.targets},\n",
        "        'test': {'data': test_set.data, 'labels': test_set.targets}\n",
        "    }\n",
        "\n",
        "#####################\n",
        "## data loading\n",
        "#####################\n",
        "\n",
        "class Batches():\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.set_random_choices = set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.set_random_choices:\n",
        "            self.dataset.set_random_choices()\n",
        "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)"
      ],
      "metadata": {
        "id": "wOKenZayrkzt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1HVsGSmsrsNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义全局变量\n",
        "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()  # 你需要定义 cifar10_mean\n",
        "std = torch.tensor(cifar10_std).view(3,1,1).cuda()  # 你需要定义 cifar10_std\n",
        "\n",
        "upper_limit, lower_limit = 1, 0\n"
      ],
      "metadata": {
        "id": "0BVFKvIPrseY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义 normalize 函数\n",
        "def normalize(X):\n",
        "    return (X - mu) / std  # 返回标准化后的张量\n",
        "\n",
        "# 定义 clamp 函数\n",
        "def clamp(X, lower_limit, upper_limit):\n",
        "    return torch.max(torch.min(X, upper_limit), lower_limit)  # 返回限制后的张量\n",
        "\n",
        "class Batches():  # 定义一个名为Batches的类，用于封装PyTorch的DataLoader\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):  # 初始化函数\n",
        "        self.dataset = dataset  # 将传入的数据集赋值给self.dataset\n",
        "        self.batch_size = batch_size  # 将传入的批处理大小赋值给self.batch_size\n",
        "        self.set_random_choices = set_random_choices  # 将传入的set_random_choices赋值给self.set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(  # 创建一个PyTorch数据加载器\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "\n",
        "    def __iter__(self):  # 定义迭代器函数\n",
        "        if self.set_random_choices:  # 如果set_random_choices为True\n",
        "            self.dataset.set_random_choices()  # 调用数据集的set_random_choices方法\n",
        "        return ({'input': x.to(device).float(), 'target': y.to(device).long()} for (x,y) in self.dataloader)  # 返回一个生成器，用于迭代数据\n",
        "\n",
        "    def __len__(self):  # 定义获取长度的函数\n",
        "        return len(self.dataloader)  # 返回数据加载器的长度\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):  # 定义名为mixup_data的函数，接受输入x, y和alpha，用于实现Mixup数据增强技术\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''  # 函数文档字符串，描述函数的功能\n",
        "    if alpha > 0:  # 判断alpha是否大于0\n",
        "        lam = np.random.beta(alpha, alpha)  # 如果是，从Beta分布中随机生成一个lambda值\n",
        "    else:\n",
        "        lam = 1  # 否则，设置lambda为1\n",
        "\n",
        "    batch_size = x.size()[0]  # 获取输入x的批量大小\n",
        "    index = torch.randperm(batch_size).cuda()  # 随机生成一个索引排列并移至GPU\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]  # 计算混合后的输入\n",
        "    y_a, y_b = y, y[index]  # 获取原始和排列后的目标标签\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam  # 返回混合后的输入、目标标签对和lambda值\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):  # 定义名为mixup_criterion的函数，接受损失函数、预测值、两组标签和lambda值，用于计算使用Mixup数据增强技术后的损失值\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)  # 返回混合后的损失值\n",
        "\n",
        "\n",
        "def attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts,\n",
        "               norm, early_stop=False,\n",
        "               mixup=False, y_a=None, y_b=None, lam=None):  # 定义名为attack_pgd的函数，用于执行投影梯度下降（PGD）攻击\n",
        "    max_loss = torch.zeros(y.shape[0]).cuda()  # 初始化最大损失为0\n",
        "    max_delta = torch.zeros_like(X).cuda()  # 初始化最大扰动为0\n",
        "    for _ in range(restarts):  # 进行多次重启以找到最有效的攻击\n",
        "        delta = torch.zeros_like(X).cuda()  # 初始化扰动为0\n",
        "        # 根据范数类型初始化扰动\n",
        "        if norm == \"l_inf\":\n",
        "            delta.uniform_(-epsilon, epsilon)\n",
        "        elif norm == \"l_2\":\n",
        "            delta.normal_()\n",
        "            d_flat = delta.view(delta.size(0),-1)\n",
        "            n = d_flat.norm(p=2,dim=1).view(delta.size(0),1,1,1)\n",
        "            r = torch.zeros_like(n).uniform_(0, 1)\n",
        "            delta *= r/n*epsilon\n",
        "        else:\n",
        "            raise ValueError  # 如果范数类型未知，则抛出错误\n",
        "        delta = clamp(delta, lower_limit-X, upper_limit-X)  # 限制扰动在允许的范围内\n",
        "        delta.requires_grad = True  # 设置delta为需要梯度\n",
        "\n",
        "        # 执行多次梯度下降步骤以找到有效的扰动\n",
        "        for _ in range(attack_iters):\n",
        "            output = model(normalize(X + delta))  # 计算模型输出\n",
        "            if early_stop:\n",
        "                index = torch.where(output.max(1)[1] == y)[0]  # 如果早停启用，找出模型预测正确的样本索引\n",
        "            else:\n",
        "                index = slice(None,None,None)  # 否则，使用一个全切片，表示所有样本\n",
        "            if not isinstance(index, slice) and len(index) == 0:\n",
        "                break  # 如果没有模型预测正确的样本，并且早停启用，则退出循环\n",
        "            if mixup:\n",
        "                criterion = nn.CrossEntropyLoss()  # 如果启用Mixup，设置交叉熵损失函数\n",
        "                loss = mixup_criterion(criterion, model(normalize(X+delta)), y_a, y_b, lam)  # 计算Mixup损失\n",
        "            else:\n",
        "                loss = F.cross_entropy(output, y)  # 否则，计算普通的交叉熵损失\n",
        "            loss.backward()  # 反向传播计算梯度\n",
        "            grad = delta.grad.detach()  # 获取扰动的梯度\n",
        "\n",
        "            # 更新扰动\n",
        "            d = delta[index, :, :, :]\n",
        "            g = grad[index, :, :, :]\n",
        "            x = X[index, :, :, :]\n",
        "\n",
        "            if norm == \"l_inf\":\n",
        "                d = torch.clamp(d + alpha * torch.sign(g), min=-epsilon, max=epsilon)\n",
        "            elif norm == \"l_2\":\n",
        "                g_norm = torch.norm(g.view(g.shape[0],-1),dim=1).view(-1,1,1,1)\n",
        "                scaled_g = g/(g_norm + 1e-10)\n",
        "                d = (d + scaled_g*alpha).view(d.size(0),-1).renorm(p=2,dim=0,maxnorm=epsilon).view_as(d)\n",
        "            d = clamp(d, lower_limit - x, upper_limit - x)\n",
        "            delta.data[index, :, :, :] = d\n",
        "            delta.grad.zero_()  # 清零梯度\n",
        "\n",
        "        # 计算使用该扰动的损失\n",
        "        if mixup:\n",
        "            criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "            all_loss = mixup_criterion(criterion, model(normalize(X+delta)), y_a, y_b, lam)\n",
        "        else:\n",
        "            all_loss = F.cross_entropy(model(normalize(X+delta)), y, reduction='none')\n",
        "\n",
        "        # 更新最大损失和对应的扰动\n",
        "        max_delta[all_loss >= max_loss] = delta.detach()[all_loss >= max_loss]\n",
        "        max_loss = torch.max(max_loss, all_loss)\n",
        "    return max_delta  # 返回找到的最有效扰动\n",
        "\n",
        "def get_args():  # 定义一个名为get_args的函数，用于获取命令行参数\n",
        "    parser = argparse.ArgumentParser()  # 创建一个ArgumentParser对象\n",
        "    parser.add_argument('--model', default='PreActResNet18')  # 指定模型类型，默认为'PreActResNet18'\n",
        "    parser.add_argument('--l2', default=0, type=float)  # L2正则化系数，默认为0\n",
        "    parser.add_argument('--l1', default=0, type=float)  # L1正则化系数，默认为0\n",
        "    parser.add_argument('--batch-size', default=128, type=int)  # 批处理大小，默认为128\n",
        "    parser.add_argument('--data-dir', default='../cifar-data', type=str)  # 数据目录，默认为'../cifar-data'\n",
        "    parser.add_argument('--epochs', default=200, type=int)  # 训练周期数，默认为200\n",
        "    parser.add_argument('--lr-schedule', default='piecewise', choices=['superconverge', 'piecewise', 'linear', 'piecewisesmoothed', 'piecewisezoom', 'onedrop', 'multipledecay', 'cosine'])  # 学习率调度策略，默认为'piecewise'\n",
        "    parser.add_argument('--lr-max', default=0.1, type=float)  # 最大学习率，默认为0.1\n",
        "    parser.add_argument('--lr-one-drop', default=0.01, type=float)  # 单次下降的学习率，默认为0.01\n",
        "    parser.add_argument('--lr-drop-epoch', default=100, type=int)  # 学习率下降的周期，默认为100\n",
        "    parser.add_argument('--attack', default='pgd', type=str, choices=['pgd', 'fgsm', 'free', 'none'])  # 攻击类型，默认为'pgd'\n",
        "    parser.add_argument('--epsilon', default=8, type=int)  # 攻击强度（epsilon值），默认为8\n",
        "    parser.add_argument('--attack-iters', default=10, type=int)  # 攻击强度（epsilon值），默认为8\n",
        "    parser.add_argument('--restarts', default=1, type=int)  # 攻击重启次数，默认为1\n",
        "    parser.add_argument('--pgd-alpha', default=2, type=float)  # PGD攻击的alpha值，默认为2\n",
        "    parser.add_argument('--fgsm-alpha', default=1.25, type=float)  # FGSM攻击的alpha值，默认为1.25\n",
        "    parser.add_argument('--norm', default='l_inf', type=str, choices=['l_inf', 'l_2'])  # 范数类型，默认为'l_inf'\n",
        "    parser.add_argument('--fgsm-init', default='random', choices=['zero', 'random', 'previous'])  # FGSM的初始化策略，默认为'random'\n",
        "    parser.add_argument('--fname', default='cifar_model', type=str)  # 输出模型的文件名，默认为'cifar_model'\n",
        "    parser.add_argument('--seed', default=0, type=int)  # 随机种子，默认为0\n",
        "    parser.add_argument('--half', action='store_true')  # 是否使用半精度浮点数，这是一个标志参数\n",
        "    parser.add_argument('--width-factor', default=10, type=int)  # 模型宽度因子，默认为10\n",
        "    parser.add_argument('--resume', default=0, type=int)  # 是否从先前的检查点恢复，默认为0（不恢复）\n",
        "    parser.add_argument('--cutout', action='store_true')  # 是否使用Cutout数据增强，这是一个标志参数\n",
        "    parser.add_argument('--cutout-len', type=int)  # Cutout的长度，如果提供\n",
        "    parser.add_argument('--mixup', action='store_true')  # 是否使用Mixup数据增强，这是一个标志参数\n",
        "    parser.add_argument('--mixup-alpha', type=float)  # Mixup的alpha值，如果提供\n",
        "    parser.add_argument('--eval', action='store_true')  # 是否处于评估模式，这是一个标志参数\n",
        "    parser.add_argument('--val', action='store_true')  #是否使用验证集，这是一个标志参数\n",
        "    parser.add_argument('--chkpt-iters', default=10, type=int)  # 检查点保存的迭代间隔，默认为10\n",
        "    return parser.parse_args()  # 解析命令行参数并返回\n"
      ],
      "metadata": {
        "id": "3NWgOtzor3xA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义参数\n",
        "class Args:\n",
        "    model = 'PreActResNet18'\n",
        "    l2 = 0.0\n",
        "    l1 = 0.0\n",
        "    batch_size = 128\n",
        "    data_dir = '../cifar-data'\n",
        "    epochs = 200\n",
        "    lr_schedule = 'piecewise'\n",
        "    lr_max = 0.1\n",
        "    lr_one_drop = 0.01\n",
        "    lr_drop_epoch = 100\n",
        "    attack = 'pgd'\n",
        "    epsilon = 8\n",
        "    attack_iters = 10\n",
        "    restarts = 1\n",
        "    pgd_alpha = 2\n",
        "    fgsm_alpha = 1.25\n",
        "    norm = 'l_inf'\n",
        "    fgsm_init = 'random'\n",
        "    fname = 'cifar_model'\n",
        "    seed = 0\n",
        "    half = False\n",
        "    width_factor = 10\n",
        "    resume = 0\n",
        "    cutout = False\n",
        "    cutout_len = None  # 需要设置具体值\n",
        "    mixup = False\n",
        "    mixup_alpha = None  # 需要设置具体值\n",
        "    eval = False\n",
        "    val = False\n",
        "    chkpt_iters = 10"
      ],
      "metadata": {
        "id": "CMir3dbjsAQZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    args = Args()  # 获取命令行参数\n",
        "\n",
        "    if not os.path.exists(args.fname):  # 检查输出目录是否存在\n",
        "        os.makedirs(args.fname)\n",
        "\n",
        "    # 初始化日志记录器\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logging.basicConfig(\n",
        "        format='[%(asctime)s] - %(message)s',\n",
        "        datefmt='%Y/%m/%d %H:%M:%S',\n",
        "        level=logging.DEBUG,\n",
        "        handlers=[\n",
        "            logging.FileHandler(os.path.join(args.fname, 'eval.log' if args.eval else 'output.log')),\n",
        "            logging.StreamHandler()\n",
        "        ])\n",
        "\n",
        "    logger.info(args)  # 记录命令行参数\n",
        "\n",
        "    # 设置随机种子以保证可重复性\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    # 初始化数据转换和数据集\n",
        "    transforms = [Crop(32, 32), FlipLR()]\n",
        "    if args.cutout:\n",
        "        transforms.append(Cutout(args.cutout_len, args.cutout_len))\n",
        "    if args.val:\n",
        "        try:\n",
        "            dataset = torch.load(\"cifar10_validation_split.pth\")\n",
        "        except:\n",
        "            print(\"Couldn't find a dataset with a validation split, did you run \"\n",
        "                  \"generate_validation.py?\")\n",
        "            return\n",
        "        val_set = list(zip(transpose(dataset['val']['data']/255.), dataset['val']['labels']))\n",
        "        val_batches = Batches(val_set, args.batch_size, shuffle=False, num_workers=2)\n",
        "    else:\n",
        "        dataset = cifar10(args.data_dir)\n",
        "    train_set = list(zip(transpose(pad(dataset['train']['data'], 4)/255.),\n",
        "        dataset['train']['labels']))\n",
        "    train_set_x = Transform(train_set, transforms)\n",
        "    train_batches = Batches(train_set_x, args.batch_size, shuffle=True, set_random_choices=True, num_workers=2)\n",
        "\n",
        "    test_set = list(zip(transpose(dataset['test']['data']/255.), dataset['test']['labels']))\n",
        "    test_batches = Batches(test_set, args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 设置攻击参数\n",
        "    epsilon = (args.epsilon / 255.)\n",
        "    pgd_alpha = (args.pgd_alpha / 255.)\n",
        "\n",
        "    # 初始化模型\n",
        "    if args.model == 'PreActResNet18':\n",
        "        model = PreActResNet18()\n",
        "    elif args.model == 'WideResNet':\n",
        "        model = WideResNet(34, 10, widen_factor=args.width_factor, dropRate=0.0)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")\n",
        "\n",
        "    model = nn.DataParallel(model).cuda()  # 使用数据并行处理\n",
        "    model.train()  # 设置模型为训练模式\n",
        "\n",
        "    # 初始化优化器和损失函数\n",
        "    if args.l2:\n",
        "        decay, no_decay = [], []\n",
        "        for name,param in model.named_parameters():\n",
        "            if 'bn' not in name and 'bias' not in name:\n",
        "                decay.append(param)\n",
        "            else:\n",
        "                no_decay.append(param)\n",
        "        params = [{'params':decay, 'weight_decay':args.l2},\n",
        "                  {'params':no_decay, 'weight_decay': 0 }]\n",
        "    else:\n",
        "        params = model.parameters()\n",
        "\n",
        "    opt = torch.optim.SGD(params, lr=args.lr_max, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 如果使用 'free' 或 'fgsm' 攻击，初始化攻击变量\n",
        "    if args.attack == 'free':\n",
        "        delta = torch.zeros(args.batch_size, 3, 32, 32).cuda()\n",
        "        delta.requires_grad = True\n",
        "    elif args.attack == 'fgsm' and args.fgsm_init == 'previous':\n",
        "        delta = torch.zeros(args.batch_size, 3, 32, 32).cuda()\n",
        "        delta.requires_grad = True\n",
        "\n",
        "    if args.attack == 'free':\n",
        "        epochs = int(math.ceil(args.epochs / args.attack_iters))\n",
        "    else:\n",
        "        epochs = args.epochs\n",
        "\n",
        "    # 初始化学习率计划\n",
        "    if args.lr_schedule == 'superconverge':\n",
        "        lr_schedule = lambda t: np.interp([t], [0, args.epochs * 2 // 5, args.epochs], [0, args.lr_max, 0])[0]\n",
        "    elif args.lr_schedule == 'piecewise':\n",
        "        def lr_schedule(t):\n",
        "            if t / args.epochs < 0.5:\n",
        "                return args.lr_max\n",
        "            elif t / args.epochs < 0.75:\n",
        "                return args.lr_max / 10.\n",
        "            else:\n",
        "                return args.lr_max / 100.\n",
        "    elif args.lr_schedule == 'linear':\n",
        "        lr_schedule = lambda t: np.interp([t], [0, args.epochs // 3, args.epochs * 2 // 3, args.epochs], [args.lr_max, args.lr_max, args.lr_max / 10, args.lr_max / 100])[0]\n",
        "    elif args.lr_schedule == 'onedrop':\n",
        "        def lr_schedule(t):\n",
        "            if t < args.lr_drop_epoch:\n",
        "                return args.lr_max\n",
        "            else:\n",
        "                return args.lr_one_drop\n",
        "    elif args.lr_schedule == 'multipledecay':\n",
        "        def lr_schedule(t):\n",
        "            return args.lr_max - (t//(args.epochs//10))*(args.lr_max/10)\n",
        "    elif args.lr_schedule == 'cosine':\n",
        "        def lr_schedule(t):\n",
        "            return args.lr_max * 0.5 * (1 + np.cos(t / args.epochs * np.pi))\n",
        "\n",
        "    # 初始化最佳准确度变量\n",
        "    best_test_robust_acc = 0\n",
        "    best_val_robust_acc = 0\n",
        "    # 从检查点恢复\n",
        "    if args.resume:\n",
        "        start_epoch = args.resume\n",
        "        model.load_state_dict(torch.load(os.path.join(args.fname, f'model_{start_epoch-1}.pth')))\n",
        "        opt.load_state_dict(torch.load(os.path.join(args.fname, f'opt_{start_epoch-1}.pth')))\n",
        "        logger.info(f'Resuming at epoch {start_epoch}')\n",
        "\n",
        "        best_test_robust_acc = torch.load(os.path.join(args.fname, f'model_best.pth'))['test_robust_acc']\n",
        "        if args.val:\n",
        "            best_val_robust_acc = torch.load(os.path.join(args.fname, f'model_val.pth'))['val_robust_acc']\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    # 主训练和评估循环\n",
        "    if args.eval:\n",
        "        if not args.resume:\n",
        "            logger.info(\"No model loaded to evaluate, specify with --resume FNAME\")\n",
        "            return\n",
        "        logger.info(\"[Evaluation mode]\")\n",
        "\n",
        "    logger.info('Epoch \\t Train Time \\t Test Time \\t LR \\t \\t Train Loss \\t Train Acc \\t Train Robust Loss \\t Train Robust Acc \\t Test Loss \\t Test Acc \\t Test Robust Loss \\t Test Robust Acc')\n",
        "\n",
        "    # 主训练和评估循环开始\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()  # 设置模型为训练模式\n",
        "        start_time = time.time()  # 记录训练开始时间\n",
        "\n",
        "        # 初始化训练统计变量\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        train_robust_loss = 0\n",
        "        train_robust_acc = 0\n",
        "        train_n = 0\n",
        "\n",
        "        # 训练批次循环\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            if args.eval:  # 如果是评估模式，跳出循环\n",
        "                break\n",
        "            X, y = batch['input'], batch['target']  # 获取输入和标签\n",
        "\n",
        "            # 如果使用Mixup数据增强\n",
        "            if args.mixup:\n",
        "                X, y_a, y_b, lam = mixup_data(X, y, args.mixup_alpha)\n",
        "                X, y_a, y_b = map(Variable, (X, y_a, y_b))\n",
        "\n",
        "            # 更新学习率\n",
        "            lr = lr_schedule(epoch + (i + 1) / len(train_batches))\n",
        "            opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "            # 执行攻击（如果有）\n",
        "            if args.attack == 'pgd':\n",
        "                # Random initialization\n",
        "                if args.mixup:\n",
        "                    delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm, mixup=True, y_a=y_a, y_b=y_b, lam=lam)\n",
        "                else:\n",
        "                    delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm)\n",
        "                delta = delta.detach()\n",
        "            elif args.attack == 'fgsm':\n",
        "                delta = attack_pgd(model, X, y, epsilon, args.fgsm_alpha*epsilon, 1, 1, args.norm)\n",
        "            # Standard training\n",
        "            elif args.attack == 'none':\n",
        "                delta = torch.zeros_like(X)\n",
        "\n",
        "            # 计算模型输出和损失\n",
        "            robust_output = model(normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
        "            if args.mixup:\n",
        "                robust_loss = mixup_criterion(criterion, robust_output, y_a, y_b, lam)\n",
        "            else:\n",
        "                robust_loss = criterion(robust_output, y)\n",
        "\n",
        "            if args.l1:\n",
        "                for name,param in model.named_parameters():\n",
        "                    if 'bn' not in name and 'bias' not in name:\n",
        "                        robust_loss += args.l1*param.abs().sum()\n",
        "\n",
        "            # 更新优化器和模型权重\n",
        "            opt.zero_grad()\n",
        "            robust_loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # 更新训练统计变量\n",
        "            output = model(normalize(X))\n",
        "            if args.mixup:\n",
        "                loss = mixup_criterion(criterion, output, y_a, y_b, lam)\n",
        "            else:\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "            train_robust_loss += robust_loss.item() * y.size(0)\n",
        "            train_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
        "            train_loss += loss.item() * y.size(0)\n",
        "            train_acc += (output.max(1)[1] == y).sum().item()\n",
        "            train_n += y.size(0)\n",
        "\n",
        "        # 记录训练结束时间\n",
        "        train_time = time.time()\n",
        "\n",
        "        model.eval()# 设置模型为评估模式\n",
        "\n",
        "        # 初始化测试统计变量\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        test_robust_loss = 0\n",
        "        test_robust_acc = 0\n",
        "        test_n = 0\n",
        "\n",
        "        # 测试批次循环\n",
        "        for i, batch in enumerate(test_batches):\n",
        "            X, y = batch['input'], batch['target']\n",
        "\n",
        "            # Random initialization\n",
        "            if args.attack == 'none':\n",
        "                delta = torch.zeros_like(X)\n",
        "            else:\n",
        "                delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm, early_stop=args.eval)\n",
        "            delta = delta.detach()\n",
        "\n",
        "            # 计算模型输出和损失\n",
        "            robust_output = model(normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
        "            robust_loss = criterion(robust_output, y)\n",
        "\n",
        "            output = model(normalize(X))\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            # 更新测试统计变量\n",
        "            test_robust_loss += robust_loss.item() * y.size(0)\n",
        "            test_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
        "            test_loss += loss.item() * y.size(0)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            test_n += y.size(0)\n",
        "\n",
        "        test_time = time.time()  # 记录测试结束时间\n",
        "\n",
        "        if args.val:\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            val_robust_loss = 0\n",
        "            val_robust_acc = 0\n",
        "            val_n = 0\n",
        "            for i, batch in enumerate(val_batches):\n",
        "                X, y = batch['input'], batch['target']\n",
        "\n",
        "                # Random initialization\n",
        "                if args.attack == 'none':\n",
        "                    delta = torch.zeros_like(X)\n",
        "                else:\n",
        "                    delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm, early_stop=args.eval)\n",
        "                delta = delta.detach()\n",
        "\n",
        "                robust_output = model(normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
        "                robust_loss = criterion(robust_output, y)\n",
        "\n",
        "                output = model(normalize(X))\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "                val_robust_loss += robust_loss.item() * y.size(0)\n",
        "                val_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
        "                val_loss += loss.item() * y.size(0)\n",
        "                val_acc += (output.max(1)[1] == y).sum().item()\n",
        "                val_n += y.size(0)\n",
        "\n",
        "        if not args.eval:\n",
        "            logger.info('%d \\t %.1f \\t \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f \\t \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f',\n",
        "                epoch, train_time - start_time, test_time - train_time, lr,\n",
        "                train_loss/train_n, train_acc/train_n, train_robust_loss/train_n, train_robust_acc/train_n,\n",
        "                test_loss/test_n, test_acc/test_n, test_robust_loss/test_n, test_robust_acc/test_n)\n",
        "\n",
        "            if args.val:\n",
        "                logger.info('validation %.4f \\t %.4f \\t %.4f \\t %.4f',\n",
        "                    val_loss/val_n, val_acc/val_n, val_robust_loss/val_n, val_robust_acc/val_n)\n",
        "\n",
        "                if val_robust_acc/val_n > best_val_robust_acc:\n",
        "                    torch.save({\n",
        "                            'state_dict':model.state_dict(),\n",
        "                            'test_robust_acc':test_robust_acc/test_n,\n",
        "                            'test_robust_loss':test_robust_loss/test_n,\n",
        "                            'test_loss':test_loss/test_n,\n",
        "                            'test_acc':test_acc/test_n,\n",
        "                            'val_robust_acc':val_robust_acc/val_n,\n",
        "                            'val_robust_loss':val_robust_loss/val_n,\n",
        "                            'val_loss':val_loss/val_n,\n",
        "                            'val_acc':val_acc/val_n,\n",
        "                        }, os.path.join(args.fname, f'model_val.pth'))\n",
        "                    best_val_robust_acc = val_robust_acc/val_n\n",
        "\n",
        "            # save checkpoint\n",
        "            if (epoch+1) % args.chkpt_iters == 0 or epoch+1 == epochs:\n",
        "                torch.save(model.state_dict(), os.path.join(args.fname, f'model_{epoch}.pth'))\n",
        "                torch.save(opt.state_dict(), os.path.join(args.fname, f'opt_{epoch}.pth'))\n",
        "\n",
        "            # save best\n",
        "            if test_robust_acc/test_n > best_test_robust_acc:\n",
        "                torch.save({\n",
        "                        'state_dict':model.state_dict(),\n",
        "                        'test_robust_acc':test_robust_acc/test_n,\n",
        "                        'test_robust_loss':test_robust_loss/test_n,\n",
        "                        'test_loss':test_loss/test_n,\n",
        "                        'test_acc':test_acc/test_n,\n",
        "                    }, os.path.join(args.fname, f'model_best.pth'))\n",
        "                best_test_robust_acc = test_robust_acc/test_n\n",
        "        else:\n",
        "            logger.info('%d \\t %.1f \\t \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f \\t \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f',\n",
        "                epoch, train_time - start_time, test_time - train_time, -1,\n",
        "                -1, -1, -1, -1,\n",
        "                test_loss/test_n, test_acc/test_n, test_robust_loss/test_n, test_robust_acc/test_n)\n",
        "            return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KehAGQ7Csbxg",
        "outputId": "4d6dfe98-7bd2-43c6-d743-09b97b0faf50"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../cifar-data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 40523081.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../cifar-data/cifar-10-python.tar.gz to ../cifar-data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    }
  ]
}